{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_LeNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nupursjsu/Deep-Learning/blob/master/Graded_Assignment4/PyTorch_LeNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gghViXQsTYj",
        "colab_type": "text"
      },
      "source": [
        "# Implementing LeNet using PyTorch (Using MNIST dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNJpsu7Tsnhz",
        "colab_type": "text"
      },
      "source": [
        "## Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSOF1FkvsnP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9HOeJwTs-lt",
        "colab_type": "text"
      },
      "source": [
        "## Loading MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAUyQh3vsGZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(train_batch_size, test_batch_size):\n",
        "    # Fetch training data: total 60000 samples\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('data', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize((32, 32)),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "    # Fetch test data: total 10000 samples\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "            transforms.Resize((32, 32)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])),\n",
        "        batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "    return (train_loader, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq4XCk-nt3T4",
        "colab_type": "text"
      },
      "source": [
        "## Defining LeNet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sShCxgJxtzyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ4X3rxgvqpB",
        "colab_type": "text"
      },
      "source": [
        "## Defining the train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czF6_Jj9vw9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, epoch, train_loader, log_interval):  \n",
        "    model.train()\n",
        "\n",
        "    #Defining loss function\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    #Iterating over batches of data\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        \n",
        "        data, target = Variable(data), Variable(target)\n",
        "\n",
        "        #Setting the gradients to zero, since PyTorch accumulates them\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #Defining Forward propagation\n",
        "        output = model(data)\n",
        "\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        #Defining Backward propagation\n",
        "        loss.backward()\n",
        "\n",
        "        #Updating the parameters after backward prop\n",
        "        optimizer.step()\n",
        "\n",
        "        #Displaying the log\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train set, Epoch {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                       100. * batch_idx / len(train_loader),\n",
        "                loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svFVHQjESBMI",
        "colab_type": "text"
      },
      "source": [
        "##Defining the test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIS0vTMgSCOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, epoch, test_loader):\n",
        "    #Defining that we are testing the model\n",
        "    model.eval()\n",
        "\n",
        "    #Initializing the variables for init loss & correct prediction accumulators\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    #Defining loss function\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(size_average=False)\n",
        "\n",
        "    #Iterating over data\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        \n",
        "        #Defining Forward propagation\n",
        "        output = model(data)\n",
        "\n",
        "        #Calculating & accumulating loss\n",
        "        test_loss += loss_fn(output, target).item()\n",
        "\n",
        "        #Getting the index of the max log-probability which is the predicted output label\n",
        "        pred = np.argmax(output.data, axis=1)\n",
        "\n",
        "        #If correct, increment correct prediction accumulator\n",
        "        correct = correct + np.equal(pred, target.data).sum()\n",
        "\n",
        "    #Displaying the log\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set, Epoch {} , Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(epoch,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bewAhXVVSkIZ",
        "colab_type": "text"
      },
      "source": [
        "##Defining the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxw9EK-_SP5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "model = LeNet()\n",
        "\n",
        "lr = 0.01\n",
        "momentum=0.5\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "train_batch_size = 64\n",
        "test_batch_size = 1000\n",
        "train_loader, test_loader = load_data(train_batch_size, test_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFjxERvhSxgv",
        "colab_type": "text"
      },
      "source": [
        "##Training and Testing the LeNet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gtb3Y41StqY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "029003a5-1d50-4e37-89ce-0310fb236f44"
      },
      "source": [
        "epochs = 10\n",
        "log_interval = 100\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, optimizer, epoch, train_loader, log_interval=log_interval)\n",
        "    test(model, epoch, test_loader)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set, Epoch 1 [0/60000 (0%)]\tLoss: 2.297229\n",
            "Train set, Epoch 1 [6400/60000 (11%)]\tLoss: 2.138053\n",
            "Train set, Epoch 1 [12800/60000 (21%)]\tLoss: 0.612845\n",
            "Train set, Epoch 1 [19200/60000 (32%)]\tLoss: 0.288354\n",
            "Train set, Epoch 1 [25600/60000 (43%)]\tLoss: 0.421566\n",
            "Train set, Epoch 1 [32000/60000 (53%)]\tLoss: 0.266445\n",
            "Train set, Epoch 1 [38400/60000 (64%)]\tLoss: 0.257292\n",
            "Train set, Epoch 1 [44800/60000 (75%)]\tLoss: 0.118953\n",
            "Train set, Epoch 1 [51200/60000 (85%)]\tLoss: 0.157034\n",
            "Train set, Epoch 1 [57600/60000 (96%)]\tLoss: 0.124597\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set, Epoch 1 , Average loss: 0.1271, Accuracy: 9595/10000 (96%)\n",
            "\n",
            "Train set, Epoch 2 [0/60000 (0%)]\tLoss: 0.122956\n",
            "Train set, Epoch 2 [6400/60000 (11%)]\tLoss: 0.056236\n",
            "Train set, Epoch 2 [12800/60000 (21%)]\tLoss: 0.260173\n",
            "Train set, Epoch 2 [19200/60000 (32%)]\tLoss: 0.111177\n",
            "Train set, Epoch 2 [25600/60000 (43%)]\tLoss: 0.087242\n",
            "Train set, Epoch 2 [32000/60000 (53%)]\tLoss: 0.079405\n",
            "Train set, Epoch 2 [38400/60000 (64%)]\tLoss: 0.154342\n",
            "Train set, Epoch 2 [44800/60000 (75%)]\tLoss: 0.043412\n",
            "Train set, Epoch 2 [51200/60000 (85%)]\tLoss: 0.111378\n",
            "Train set, Epoch 2 [57600/60000 (96%)]\tLoss: 0.094715\n",
            "\n",
            "Test set, Epoch 2 , Average loss: 0.1065, Accuracy: 9654/10000 (97%)\n",
            "\n",
            "Train set, Epoch 3 [0/60000 (0%)]\tLoss: 0.141730\n",
            "Train set, Epoch 3 [6400/60000 (11%)]\tLoss: 0.028807\n",
            "Train set, Epoch 3 [12800/60000 (21%)]\tLoss: 0.060322\n",
            "Train set, Epoch 3 [19200/60000 (32%)]\tLoss: 0.055607\n",
            "Train set, Epoch 3 [25600/60000 (43%)]\tLoss: 0.076396\n",
            "Train set, Epoch 3 [32000/60000 (53%)]\tLoss: 0.041063\n",
            "Train set, Epoch 3 [38400/60000 (64%)]\tLoss: 0.085574\n",
            "Train set, Epoch 3 [44800/60000 (75%)]\tLoss: 0.026864\n",
            "Train set, Epoch 3 [51200/60000 (85%)]\tLoss: 0.027914\n",
            "Train set, Epoch 3 [57600/60000 (96%)]\tLoss: 0.035504\n",
            "\n",
            "Test set, Epoch 3 , Average loss: 0.0654, Accuracy: 9803/10000 (98%)\n",
            "\n",
            "Train set, Epoch 4 [0/60000 (0%)]\tLoss: 0.028410\n",
            "Train set, Epoch 4 [6400/60000 (11%)]\tLoss: 0.029777\n",
            "Train set, Epoch 4 [12800/60000 (21%)]\tLoss: 0.025189\n",
            "Train set, Epoch 4 [19200/60000 (32%)]\tLoss: 0.214681\n",
            "Train set, Epoch 4 [25600/60000 (43%)]\tLoss: 0.116581\n",
            "Train set, Epoch 4 [32000/60000 (53%)]\tLoss: 0.042698\n",
            "Train set, Epoch 4 [38400/60000 (64%)]\tLoss: 0.015656\n",
            "Train set, Epoch 4 [44800/60000 (75%)]\tLoss: 0.124302\n",
            "Train set, Epoch 4 [51200/60000 (85%)]\tLoss: 0.061978\n",
            "Train set, Epoch 4 [57600/60000 (96%)]\tLoss: 0.046806\n",
            "\n",
            "Test set, Epoch 4 , Average loss: 0.0564, Accuracy: 9819/10000 (98%)\n",
            "\n",
            "Train set, Epoch 5 [0/60000 (0%)]\tLoss: 0.044701\n",
            "Train set, Epoch 5 [6400/60000 (11%)]\tLoss: 0.027658\n",
            "Train set, Epoch 5 [12800/60000 (21%)]\tLoss: 0.052783\n",
            "Train set, Epoch 5 [19200/60000 (32%)]\tLoss: 0.037395\n",
            "Train set, Epoch 5 [25600/60000 (43%)]\tLoss: 0.238857\n",
            "Train set, Epoch 5 [32000/60000 (53%)]\tLoss: 0.037771\n",
            "Train set, Epoch 5 [38400/60000 (64%)]\tLoss: 0.091367\n",
            "Train set, Epoch 5 [44800/60000 (75%)]\tLoss: 0.025187\n",
            "Train set, Epoch 5 [51200/60000 (85%)]\tLoss: 0.171236\n",
            "Train set, Epoch 5 [57600/60000 (96%)]\tLoss: 0.029826\n",
            "\n",
            "Test set, Epoch 5 , Average loss: 0.0542, Accuracy: 9817/10000 (98%)\n",
            "\n",
            "Train set, Epoch 6 [0/60000 (0%)]\tLoss: 0.018519\n",
            "Train set, Epoch 6 [6400/60000 (11%)]\tLoss: 0.059845\n",
            "Train set, Epoch 6 [12800/60000 (21%)]\tLoss: 0.025090\n",
            "Train set, Epoch 6 [19200/60000 (32%)]\tLoss: 0.023249\n",
            "Train set, Epoch 6 [25600/60000 (43%)]\tLoss: 0.068763\n",
            "Train set, Epoch 6 [32000/60000 (53%)]\tLoss: 0.026272\n",
            "Train set, Epoch 6 [38400/60000 (64%)]\tLoss: 0.015383\n",
            "Train set, Epoch 6 [44800/60000 (75%)]\tLoss: 0.023115\n",
            "Train set, Epoch 6 [51200/60000 (85%)]\tLoss: 0.096826\n",
            "Train set, Epoch 6 [57600/60000 (96%)]\tLoss: 0.006715\n",
            "\n",
            "Test set, Epoch 6 , Average loss: 0.0461, Accuracy: 9851/10000 (99%)\n",
            "\n",
            "Train set, Epoch 7 [0/60000 (0%)]\tLoss: 0.074042\n",
            "Train set, Epoch 7 [6400/60000 (11%)]\tLoss: 0.055926\n",
            "Train set, Epoch 7 [12800/60000 (21%)]\tLoss: 0.157031\n",
            "Train set, Epoch 7 [19200/60000 (32%)]\tLoss: 0.034175\n",
            "Train set, Epoch 7 [25600/60000 (43%)]\tLoss: 0.065114\n",
            "Train set, Epoch 7 [32000/60000 (53%)]\tLoss: 0.014378\n",
            "Train set, Epoch 7 [38400/60000 (64%)]\tLoss: 0.025236\n",
            "Train set, Epoch 7 [44800/60000 (75%)]\tLoss: 0.092153\n",
            "Train set, Epoch 7 [51200/60000 (85%)]\tLoss: 0.045958\n",
            "Train set, Epoch 7 [57600/60000 (96%)]\tLoss: 0.015903\n",
            "\n",
            "Test set, Epoch 7 , Average loss: 0.0380, Accuracy: 9871/10000 (99%)\n",
            "\n",
            "Train set, Epoch 8 [0/60000 (0%)]\tLoss: 0.013485\n",
            "Train set, Epoch 8 [6400/60000 (11%)]\tLoss: 0.010434\n",
            "Train set, Epoch 8 [12800/60000 (21%)]\tLoss: 0.010032\n",
            "Train set, Epoch 8 [19200/60000 (32%)]\tLoss: 0.029373\n",
            "Train set, Epoch 8 [25600/60000 (43%)]\tLoss: 0.097387\n",
            "Train set, Epoch 8 [32000/60000 (53%)]\tLoss: 0.044672\n",
            "Train set, Epoch 8 [38400/60000 (64%)]\tLoss: 0.029341\n",
            "Train set, Epoch 8 [44800/60000 (75%)]\tLoss: 0.045393\n",
            "Train set, Epoch 8 [51200/60000 (85%)]\tLoss: 0.002756\n",
            "Train set, Epoch 8 [57600/60000 (96%)]\tLoss: 0.012727\n",
            "\n",
            "Test set, Epoch 8 , Average loss: 0.0351, Accuracy: 9892/10000 (99%)\n",
            "\n",
            "Train set, Epoch 9 [0/60000 (0%)]\tLoss: 0.024910\n",
            "Train set, Epoch 9 [6400/60000 (11%)]\tLoss: 0.019283\n",
            "Train set, Epoch 9 [12800/60000 (21%)]\tLoss: 0.021649\n",
            "Train set, Epoch 9 [19200/60000 (32%)]\tLoss: 0.071228\n",
            "Train set, Epoch 9 [25600/60000 (43%)]\tLoss: 0.005792\n",
            "Train set, Epoch 9 [32000/60000 (53%)]\tLoss: 0.045057\n",
            "Train set, Epoch 9 [38400/60000 (64%)]\tLoss: 0.003318\n",
            "Train set, Epoch 9 [44800/60000 (75%)]\tLoss: 0.044800\n",
            "Train set, Epoch 9 [51200/60000 (85%)]\tLoss: 0.069590\n",
            "Train set, Epoch 9 [57600/60000 (96%)]\tLoss: 0.095640\n",
            "\n",
            "Test set, Epoch 9 , Average loss: 0.0455, Accuracy: 9850/10000 (98%)\n",
            "\n",
            "Train set, Epoch 10 [0/60000 (0%)]\tLoss: 0.026176\n",
            "Train set, Epoch 10 [6400/60000 (11%)]\tLoss: 0.012678\n",
            "Train set, Epoch 10 [12800/60000 (21%)]\tLoss: 0.029479\n",
            "Train set, Epoch 10 [19200/60000 (32%)]\tLoss: 0.000815\n",
            "Train set, Epoch 10 [25600/60000 (43%)]\tLoss: 0.003438\n",
            "Train set, Epoch 10 [32000/60000 (53%)]\tLoss: 0.014697\n",
            "Train set, Epoch 10 [38400/60000 (64%)]\tLoss: 0.023537\n",
            "Train set, Epoch 10 [44800/60000 (75%)]\tLoss: 0.058405\n",
            "Train set, Epoch 10 [51200/60000 (85%)]\tLoss: 0.020089\n",
            "Train set, Epoch 10 [57600/60000 (96%)]\tLoss: 0.013678\n",
            "\n",
            "Test set, Epoch 10 , Average loss: 0.0402, Accuracy: 9862/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQngZ_MqW61K",
        "colab_type": "text"
      },
      "source": [
        "##Conclusion\n",
        "\n",
        "We could see LeNet model is giving an accuracy of 99% on MNIST dataset which is pretty good."
      ]
    }
  ]
}